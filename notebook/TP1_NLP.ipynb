{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5a51a03a",
      "metadata": {
        "id": "5a51a03a"
      },
      "source": [
        "## Instalação de dependências\n",
        "\n",
        "Esta célula instala as bibliotecas Python necessárias para executar o restante do notebook, incluindo:\n",
        "\n",
        "*   `gensim`: Para trabalhar com modelos de word embeddings, como Word2Vec.\n",
        "*   `ipykernel`: Essencial para o funcionamento do ambiente de notebook.\n",
        "*   `requests`: Para fazer download de dados da internet.\n",
        "*   `pandas`: Para manipulação e análise de dados em formato de tabela.\n",
        "*   `matplotlib` e `seaborn`: Para visualização de dados.\n",
        "\n",
        "Se você estiver executando este notebook no Google Colab e for solicitado a reiniciar o ambiente de execução após a instalação, por favor, faça-o para garantir que todas as bibliotecas estejam carregadas corretamente antes de prosseguir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7a350eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7a350eb",
        "outputId": "4779059d-3ea7-44b7-f879-87e378d40e19",
        "tags": [
          "ignore"
        ]
      },
      "outputs": [],
      "source": [
        "%pip install gensim\n",
        "%pip install ipykernel\n",
        "%pip install requests\n",
        "%pip install pandas\n",
        "%pip install matplotlib\n",
        "%pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63aff825",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63aff825",
        "outputId": "a977d7db-a215-49b5-f3c7-bbd0dd04385f"
      },
      "outputs": [],
      "source": [
        "# Importação das bibliotecas necessárias\n",
        "import gensim\n",
        "\n",
        "print(f\"Gensim instalado na versão: {gensim.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4823138a",
      "metadata": {
        "id": "4823138a"
      },
      "source": [
        "## Download e Preparação dos Dados\n",
        "\n",
        "Nesta etapa, é realizado o download e a extração dos arquivos que servirão de base para o nosso estudo. O corpus de texto `text8.zip` é um conjunto de dados utilizado para o treinamento de modelos de *word embeddings*, contendo uma grande coleção de textos.\n",
        "\n",
        "Além do corpus, também baixamos o arquivo `questions-words.txt`, que é um recurso valioso para a avaliação da qualidade do nosso modelo Word2Vec. Este arquivo contém uma lista de analogias semânticas e sintáticas, que nos permitirá testar se o modelo aprendeu relações significativas entre as palavras.\n",
        "\n",
        "Todos os arquivos baixados serão armazenados em uma pasta local chamada `data`, mantendo nosso projeto organizado e os dados facilmente acessíveis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23655522",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23655522",
        "outputId": "debfc8bc-3a55-4dad-a110-ca198d31fb72"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "def download_and_extract(url, dest_path):\n",
        "    filename = url.split('/')[-1]\n",
        "    filepath = os.path.join(dest_path, filename)\n",
        "\n",
        "    if not os.path.exists(dest_path):\n",
        "        os.makedirs(dest_path)\n",
        "\n",
        "    # Verifica se o arquivo já existe (descompactado, se for zip)\n",
        "    if not os.path.exists(filepath.replace('.zip', '')):\n",
        "        print(f\"Baixando {filename}...\")\n",
        "        r = requests.get(url, stream=True)\n",
        "\n",
        "        # Salva o arquivo em chunks\n",
        "        with open(filepath, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=1024):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "        print(f\"{filename} baixado.\")\n",
        "\n",
        "        if filename.endswith('.zip'):\n",
        "            print(f\"Extraindo {filename}...\")\n",
        "            with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
        "                zip_ref.extractall(dest_path)\n",
        "            os.remove(filepath)\n",
        "            print(\"Extração completa.\")\n",
        "    else:\n",
        "        print(f\"{filename.replace('.zip', '')} já existe. Download pulado.\")\n",
        "\n",
        "# URLs dos arquivos que serão baixados\n",
        "corpus_url = 'http://mattmahoney.net/dc/text8.zip'\n",
        "analogy_url = 'https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt'\n",
        "dest_folder = '../data'\n",
        "\n",
        "# Baixar e extrair o corpus de treinamento (text8.zip)\n",
        "download_and_extract(corpus_url, dest_folder)\n",
        "corpus_path = os.path.join(dest_folder, 'text8')\n",
        "\n",
        "# Baixar o arquivo de analogias (questions-words.txt)\n",
        "download_and_extract(analogy_url, dest_folder)\n",
        "analogy_path = os.path.join(dest_folder, 'questions-words.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c53efa60",
      "metadata": {
        "id": "c53efa60"
      },
      "source": [
        "## Pré-processamento do Texto\n",
        "\n",
        "Nesta seção, o texto do corpus `text8` passa por um processo de **pré-processamento** para prepará-lo para o treinamento do modelo Word2Vec. As etapas incluem:\n",
        "\n",
        "1.  **Remoção de Stopwords:** Palavras comuns que geralmente não carregam significado semântico importante (stopwords) são removidas do texto. Isso ajuda a reduzir o ruído e focar nas palavras mais relevantes.\n",
        "2.  **Detecção de Bigramas:** O modelo `gensim.models.Phrases` é utilizado para identificar e agrupar palavras que frequentemente aparecem juntas, formando \"frases\" ou bigramas (por exemplo, \"new_york\"). Isso permite que o modelo trate essas combinações como unidades únicas, capturando melhor o seu significado contextual.\n",
        "\n",
        "O resultado dessas etapas é um corpus processado, pronto para ser usado no treinamento do modelo Word2Vec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27fd820e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27fd820e",
        "outputId": "d6294627-4656-4652-fb3d-415487ccc9c8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from gensim.models import Phrases\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "corpus_path = os.path.join(dest_folder, 'text8')\n",
        "\n",
        "# Abre o arquivo do corpus e lê todo o conteúdo, dividindo em palavras\n",
        "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
        "    words = f.read().split()\n",
        "\n",
        "CHUNK_SIZE = 10000\n",
        "\n",
        "# Divide a lista de palavras em chunks e remove stopwords de cada chunk\n",
        "chunked_sentences = [remove_stopwords(\" \".join(words[i:i + CHUNK_SIZE])) for i in range(0, len(words), CHUNK_SIZE)]\n",
        "\n",
        "# O modelo Phrases espera uma lista de listas de strings (sentenças divididas em palavras)\n",
        "processed_chunks = [sentence.split() for sentence in chunked_sentences]\n",
        "\n",
        "# Cria um modelo de bigramas para identificar frases comuns (ex: \"new_york\")\n",
        "bigram_model = Phrases(processed_chunks, min_count=5, threshold=10, progress_per=100)\n",
        "final_corpus = bigram_model[processed_chunks]\n",
        "\n",
        "sentences = list(final_corpus)\n",
        "\n",
        "print(\"\\nPreparação de dados concluída com sucesso!\")\n",
        "size = sum(len(sentence) for sentence in sentences)\n",
        "print(f\"Corpus final pronto com {size} palavras para o treinamento.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e77476f",
      "metadata": {
        "id": "7e77476f"
      },
      "source": [
        "## Pré-processamento dos Dados de Analogia\n",
        "\n",
        "Responsável por realiza um pré-processamento simples para preparar os dados para a avaliação `questions-words.txt` :\n",
        "\n",
        "1.  **Leitura do Arquivo:** O arquivo é lido linha por linha.\n",
        "2.  **Filtragem:** Linhas que começam com o caractere `:` são ignoradas, pois elas indicam categorias e não analogias.\n",
        "3.  **Tokenização e Normalização:** Para cada linha válida, o texto é dividido em palavras (tokenização), e todas as palavras são convertidas para minúsculas. Espaços em branco extras no início ou fim da linha também são removidos.\n",
        "\n",
        "Este pré-processamento garante que os dados de analogia estejam em um formato consistente e limpo, pronto para ser usado pela função de avaliação do modelo Word2Vec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3859bc06",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3859bc06",
        "outputId": "09b343e2-9a33-48eb-d50d-02dcf854fd35"
      },
      "outputs": [],
      "source": [
        "analogy_lines = []\n",
        "with open(analogy_path) as file:\n",
        "    for line in file:\n",
        "        # Ignora linhas que começam com ':'\n",
        "        if line.startswith(':'):\n",
        "            continue\n",
        "\n",
        "        # Divide a linha em palavras, converte para minúsculas e remove espaços em branco\n",
        "        line = [word.lower() for word in line.strip().split()]\n",
        "        analogy_lines.append(line)\n",
        "\n",
        "print(analogy_lines[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f79b10ac",
      "metadata": {
        "id": "f79b10ac"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorized_cosine_distance(A, B):\n",
        "    dot_product = np.einsum('ij,ij->i', A, B)\n",
        "    norm_A = np.linalg.norm(A, axis=1)\n",
        "    norm_B = np.linalg.norm(B, axis=1)\n",
        "    denominator = (norm_A * norm_B) + 1e-8\n",
        "    similarity = dot_product / denominator\n",
        "    similarity = np.clip(similarity, -1.0, 1.0)\n",
        "    return 1 - similarity\n",
        "\n",
        "def evaluate_word_analogies(model, analogies):\n",
        "    vocab = model.wv.key_to_index\n",
        "\n",
        "    valid_analogies = [\n",
        "        line for line in analogies\n",
        "        if all(word in vocab for word in line)\n",
        "    ]\n",
        "\n",
        "    if not valid_analogies:\n",
        "        return 0.0\n",
        "\n",
        "    valid_analogies_np = np.array(valid_analogies)\n",
        "\n",
        "    vecs1 = model.wv[valid_analogies_np[:, 0]] # Vetor A\n",
        "    vecs2 = model.wv[valid_analogies_np[:, 1]] # Vetor B\n",
        "    vecs3 = model.wv[valid_analogies_np[:, 2]] # Vetor C\n",
        "\n",
        "    predicted_vecs = vecs2 - vecs1 + vecs3\n",
        "\n",
        "    expected_vecs = model.wv[valid_analogies_np[:, 3]] # Vetor D (esperado)\n",
        "    distances = vectorized_cosine_distance(predicted_vecs, expected_vecs)\n",
        "\n",
        "    return np.mean(distances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5143c34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5143c34",
        "outputId": "c95e44c8-9116-42aa-fe9a-1a1ca620e0c1"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "# Definindo os hiperparâmetros para o experimento\n",
        "param_grid = {\n",
        "    'sg': [0, 1],  # 0 para CBOW, 1 para Skip-gram\n",
        "    'vector_size': [250, 300, 350], # Tamanho do embedding\n",
        "    'window': [5, 7, 10], # Janela de contexto\n",
        "    'epochs': [5, 10, 20, 30], # Iterações de treinamento\n",
        "    'alpha': [0.05, 0.025, 0.01, 0.0075] # Taxa de aprendizado inicial\n",
        "}\n",
        "\n",
        "# Gerar todas as combinações de parâmetros\n",
        "param_combinations = list(itertools.product(\n",
        "    param_grid['sg'],\n",
        "    param_grid['vector_size'],\n",
        "    param_grid['window'],\n",
        "    param_grid['epochs'],\n",
        "    param_grid['alpha']\n",
        "))\n",
        "\n",
        "print(f\"Total de combinações de parâmetros: {len(param_combinations)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ae3f33f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ae3f33f",
        "outputId": "a37a0ccd-be80-444e-87fd-0b3e38a147b7"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import time\n",
        "import os\n",
        "\n",
        "print(\"Iniciando o treinamento e avaliação dos modelos...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Semente utilizada para ter resultado comparáveis\n",
        "seed = 137\n",
        "cores = os.cpu_count() or 1\n",
        "model_count = 0\n",
        "total_models = len(param_combinations)\n",
        "\n",
        "# Define a base path for models\n",
        "model_base_path = os.path.join(dest_folder, 'word2vec_models')\n",
        "\n",
        "# Create the models directory if it doesn't exist\n",
        "if not os.path.exists(model_base_path):\n",
        "    os.makedirs(model_base_path)\n",
        "\n",
        "results = []\n",
        "\n",
        "for sg, size, window, epochs, alpha in param_combinations:\n",
        "    model_count += 1\n",
        "    start_time = time.time()\n",
        "\n",
        "    model_name = f\"sg={sg}_size={size}_window={window}_epochs={epochs}_alpha={alpha}\"\n",
        "    model_path = os.path.join(model_base_path, model_name)\n",
        "    architecture_name = 'Skip-gram' if sg == 1 else 'CBOW'\n",
        "\n",
        "    print(f\"({model_count}/{total_models}) Processando modelo: {architecture_name}, Vec Size={size}, Window={window}, Epochs={epochs}, Alpha={alpha}\")\n",
        "\n",
        "    # Check if the model already exists\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"Carregando modelo existente de: {model_path}\")\n",
        "        model = Word2Vec.load(model_path)\n",
        "    else:\n",
        "        print(f\"Treinando novo modelo: {architecture_name}, Vec Size={size}, Window={window}, Epochs={epochs}, Alpha={alpha}\")\n",
        "        model = Word2Vec(\n",
        "            sentences=sentences,\n",
        "            sg=sg,\n",
        "            vector_size=size,\n",
        "            window=window,\n",
        "            epochs=epochs,\n",
        "            min_count=5, # Ignora palavras com frequência menor que 5\n",
        "            workers=cores,\n",
        "            alpha=alpha,\n",
        "            seed=seed\n",
        "        )\n",
        "        print(\"Treinamento concluído. Salvando modelo...\")\n",
        "        model.save(model_path)\n",
        "\n",
        "    print(\"Iniciando avaliação de analogias...\")\n",
        "    mean_distance = evaluate_word_analogies(model, analogy_lines)\n",
        "    print(\"Distância média:\", mean_distance)\n",
        "\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "    print(f\"Duração do processo: {duration:.2f} segundos\")\n",
        "\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    results.append({\n",
        "        'Arquitetura': architecture_name,\n",
        "        'Tamanho Embedding': size,\n",
        "        'Janela Contexto': window,\n",
        "        'Épocas': epochs,\n",
        "        'Taxa Aprendizado': alpha,\n",
        "        'Distância média': mean_distance,\n",
        "        'Duração (s)': duration\n",
        "    })\n",
        "\n",
        "print(\"Todos os modelos foram processados e avaliados com sucesso!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e2a1cb0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "3e2a1cb0",
        "outputId": "0b036fa3-8fa9-4ac5-84ee-dd1c3f7ec349",
        "tags": [
          "ignore"
        ]
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "best_models = results_df.sort_values(by='Distância média')\n",
        "\n",
        "display(best_models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8b445ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c8b445ac",
        "outputId": "5ff3da88-3e9f-4d99-9195-bb61d69832dd",
        "tags": [
          "ignore"
        ]
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configura o estilo dos gráficos\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Gráfico 1: Comparação de Arquitetura (CBOW vs. Skip-gram)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=results_df, x='Arquitetura', y='Distância média')\n",
        "plt.title('Distância Média por Arquitetura do Modelo')\n",
        "plt.ylabel('Distância Média de Analogia')\n",
        "plt.xlabel('Arquitetura')\n",
        "plt.show()\n",
        "\n",
        "# Gráfico 2: Impacto do Tamanho do Embedding\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=results_df, x='Tamanho Embedding', y='Distância média', hue='Arquitetura', style='Arquitetura', markers=True, dashes=False)\n",
        "plt.title('Impacto do Tamanho do Embedding na Distância Média')\n",
        "plt.ylabel('Distância Média')\n",
        "plt.xlabel('Dimensões do Vetor (Embedding Size)')\n",
        "plt.legend(title='Arquitetura')\n",
        "plt.show()\n",
        "\n",
        "# Gráfico 3: Impacto do Tamanho da Janela de Contexto\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=results_df, x='Janela Contexto', y='Distância média', hue='Arquitetura', style='Arquitetura', markers=True, dashes=False)\n",
        "plt.title('Impacto da Janela de Contexto na Distância Média')\n",
        "plt.ylabel('Distância Média')\n",
        "plt.xlabel('Tamanho da Janela')\n",
        "plt.legend(title='Arquitetura')\n",
        "plt.show()\n",
        "\n",
        "# Gráfico 4: Impacto do Número de Épocas\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=results_df, x='Épocas', y='Distância média', hue='Arquitetura', style='Arquitetura', markers=True, dashes=False)\n",
        "plt.title('Impacto do Número de Épocas na Distância Média')\n",
        "plt.ylabel('Distância Média')\n",
        "plt.xlabel('Iterações de Treinamento')\n",
        "plt.legend(title='Arquitetura')\n",
        "plt.show()\n",
        "\n",
        "# Gráfico 5: Impacto da Taxa de Aprendizado\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=results_df, x='Taxa Aprendizado', y='Distância média', hue='Arquitetura', style='Arquitetura', markers=True, dashes=False)\n",
        "plt.title('Impacto da Taxa de Aprendizado na Distância Média')\n",
        "plt.ylabel('Distância Média')\n",
        "plt.xlabel('Taxa de Aprendizado Inicial (Alpha)')\n",
        "plt.legend(title='Arquitetura')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f8fcde0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Gráfico 6: Interação entre Taxa de Aprendizado (Alpha) e Épocas\n",
        "g = sns.catplot(\n",
        "    data=results_df,\n",
        "    x='Épocas',\n",
        "    y='Distância média',\n",
        "    hue='Taxa Aprendizado',\n",
        "    col='Arquitetura',\n",
        "    kind='point',\n",
        "    palette='flare',\n",
        "    height=6,\n",
        "    aspect=1.2\n",
        ")\n",
        "g.fig.suptitle('Impacto Combinado de Épocas e Taxa de Aprendizado', y=1.03)\n",
        "g.set_axis_labels('Número de Épocas', 'Distância Média')\n",
        "g.legend.set_title('Taxa de Aprendizado')\n",
        "plt.show()\n",
        "\n",
        "# Gráfico 7: Interação entre Tamanho do Embedding e Janela de Contexto\n",
        "g = sns.catplot(\n",
        "    data=results_df,\n",
        "    x='Tamanho Embedding',\n",
        "    y='Distância média',\n",
        "    hue='Janela Contexto',\n",
        "    col='Arquitetura',\n",
        "    kind='point',\n",
        "    palette='flare',\n",
        "    height=6,\n",
        "    aspect=1.2\n",
        ")\n",
        "g.fig.suptitle('Impacto Combinado do Tamanho do Embedding e Janela de Contexto', y=1.03)\n",
        "g.set_axis_labels('Tamanho do Embedding', 'Distância Média')\n",
        "g.legend.set_title('Janela de Contexto')\n",
        "plt.show()\n",
        "\n",
        "# Gráfico 8: Interação entre Épocas e Janela de Contexto\n",
        "g = sns.catplot(\n",
        "    data=results_df,\n",
        "    x='Épocas',\n",
        "    y='Distância média',\n",
        "    hue='Janela Contexto',\n",
        "    col='Arquitetura',\n",
        "    kind='point',\n",
        "    palette='flare',\n",
        "    height=6,\n",
        "    aspect=1.2\n",
        ")\n",
        "g.fig.suptitle('Impacto Combinado de Épocas e Janela de Contexto', y=1.03)\n",
        "g.set_axis_labels('Número de Épocas', 'Distância Média')\n",
        "g.legend.set_title('Janela de Contexto')\n",
        "plt.show()\n",
        "\n",
        "# Gráfico 9: Interação entre Tamanho do Embedding e Taxa de Aprendizado\n",
        "g = sns.catplot(\n",
        "    data=results_df,\n",
        "    x='Tamanho Embedding',\n",
        "    y='Distância média',\n",
        "    hue='Taxa Aprendizado',\n",
        "    col='Arquitetura',\n",
        "    kind='point',\n",
        "    palette='flare',\n",
        "    height=6,\n",
        "    aspect=1.2\n",
        ")\n",
        "g.fig.suptitle('Impacto Combinado do Embedding e Taxa de Aprendizado', y=1.03)\n",
        "g.set_axis_labels('Tamanho do Embedding', 'Distância Média')\n",
        "g.legend.set_title('Taxa de Aprendizado')\n",
        "plt.show()\n",
        "\n",
        "# Gráfico 10: Interação entre Janela de Contexto e Taxa de Aprendizado\n",
        "g = sns.catplot(\n",
        "    data=results_df,\n",
        "    x='Janela Contexto',\n",
        "    y='Distância média',\n",
        "    hue='Taxa Aprendizado',\n",
        "    col='Arquitetura',\n",
        "    kind='point',\n",
        "    palette='flare',\n",
        "    height=6,\n",
        "    aspect=1.2\n",
        ")\n",
        "g.fig.suptitle('Impacto Combinado da Janela de Contexto e Taxa de Aprendizado', y=1.03)\n",
        "g.set_axis_labels('Janela de Contexto', 'Distância Média')\n",
        "g.legend.set_title('Taxa de Aprendizado')\n",
        "plt.show()\n",
        "\n",
        "# Gráfico 11: Interação entre Épocas e Tamanho do Embedding\n",
        "g = sns.catplot(\n",
        "    data=results_df,\n",
        "    x='Épocas',\n",
        "    y='Distância média',\n",
        "    hue='Tamanho Embedding',\n",
        "    col='Arquitetura',\n",
        "    kind='point',\n",
        "    palette='flare',\n",
        "    height=6,\n",
        "    aspect=1.2\n",
        ")\n",
        "g.fig.suptitle('Impacto Combinado de Épocas e Tamanho do Embedding', y=1.03)\n",
        "g.set_axis_labels('Número de Épocas', 'Distância Média')\n",
        "g.legend.set_title('Tamanho Embedding')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
